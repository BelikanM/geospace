import React, { useRef, useEffect, useState, useCallback } from 'react';
import Webcam from 'react-webcam';
import * as cocoSsd from '@tensorflow-models/coco-ssd';
import * as tf from '@tensorflow/tfjs';
import '@tensorflow/tfjs-backend-webgl';
import { motion, AnimatePresence } from 'framer-motion';
import { FaCamera, FaSave, FaInfoCircle, FaCog, FaExchangeAlt, FaVolumeUp, FaVolumeMute, FaRobot, FaBrain } from 'react-icons/fa';
import { BsZoomIn, BsZoomOut } from 'react-icons/bs';
import { MdPhotoLibrary, MdOutlineInfo } from 'react-icons/md';
import './Analyse.css';

// On initialise avec un objet vide qui sera remplac√© par les donn√©es du fichier JSON
let objetInfos = {};

// Tableaux de noms de classes pour le mod√®le YOLO personnalis√©
const YOLO_CLASSES = [
  "personne", "v√©lo", "voiture", "moto", "avion", "bus", "train", "camion", 
  "bateau", "feu-tricolore", "bouche-incendie", "panneau-stop", "parcm√®tre", 
  "banc", "oiseau", "chat", "chien", "cheval", "mouton", "vache", "√©l√©phant", 
  "ours", "z√®bre", "girafe", "sac-√†-dos", "parapluie", "sac-√†-main", "cravate", 
  "valise", "frisbee", "skis", "snowboard", "ballon-sport", "cerf-volant", 
  "batte-baseball", "gant-baseball", "skateboard", "planche-surf", "raquette-tennis", 
  "bouteille", "verre-vin", "tasse", "fourchette", "couteau", "cuill√®re", "bol", 
  "banane", "pomme", "sandwich", "orange", "brocoli", "carotte", "hot-dog", 
  "pizza", "donut", "g√¢teau", "chaise", "canap√©", "plante-en-pot", "lit", 
  "table-√†-manger", "toilettes", "t√©l√©viseur", "ordinateur-portable", "souris", 
  "t√©l√©commande", "clavier", "t√©l√©phone", "micro-ondes", "four", "grille-pain", 
  "√©vier", "r√©frig√©rateur", "livre", "horloge", "vase", "ciseaux", "ours-en-peluche", 
  "s√®che-cheveux", "brosse-√†-dents", "√©couteurs", "cl√©", "lunettes", "montre", "stylo"
];

// Table de correspondance entre les noms anglais du mod√®le COCO-SSD et les noms fran√ßais
const COCO_TO_FRENCH = {
  "person": "personne",
  "bicycle": "v√©lo",
  "car": "voiture",
  "motorcycle": "moto",
  "airplane": "avion",
  "bus": "bus",
  "train": "train",
  "truck": "camion",
  "boat": "bateau",
  "traffic light": "feu-tricolore",
  "fire hydrant": "bouche-incendie",
  "stop sign": "panneau-stop",
  "parking meter": "parcm√®tre",
  "bench": "banc",
  "bird": "oiseau",
  "cat": "chat",
  "dog": "chien",
  "horse": "cheval",
  "sheep": "mouton",
  "cow": "vache",
  "elephant": "√©l√©phant",
  "bear": "ours",
  "zebra": "z√®bre",
  "giraffe": "girafe",
  "backpack": "sac-√†-dos",
  "umbrella": "parapluie",
  "handbag": "sac-√†-main",
  "tie": "cravate",
  "suitcase": "valise",
  "frisbee": "frisbee",
  "skis": "skis",
  "snowboard": "snowboard",
  "sports ball": "ballon-sport",
  "kite": "cerf-volant",
  "baseball bat": "batte-baseball",
  "baseball glove": "gant-baseball",
  "skateboard": "skateboard",
  "surfboard": "planche-surf",
  "tennis racket": "raquette-tennis",
  "bottle": "bouteille",
  "wine glass": "verre-vin",
  "cup": "tasse",
  "fork": "fourchette",
  "knife": "couteau",
  "spoon": "cuill√®re",
  "bowl": "bol",
  "banana": "banane",
  "apple": "pomme",
  "sandwich": "sandwich",
  "orange": "orange",
  "broccoli": "brocoli",
  "carrot": "carotte",
  "hot dog": "hot-dog",
  "pizza": "pizza",
  "donut": "donut",
  "cake": "g√¢teau",
  "chair": "chaise",
  "couch": "canap√©",
  "potted plant": "plante-en-pot",
  "bed": "lit",
  "dining table": "table-√†-manger",
  "toilet": "toilettes",
  "tv": "t√©l√©viseur",
  "laptop": "ordinateur-portable",
  "mouse": "souris",
  "remote": "t√©l√©commande",
  "keyboard": "clavier",
  "cell phone": "t√©l√©phone",
  "microwave": "micro-ondes",
  "oven": "four",
  "toaster": "grille-pain",
  "sink": "√©vier",
  "refrigerator": "r√©frig√©rateur",
  "book": "livre",
  "clock": "horloge",
  "vase": "vase",
  "scissors": "ciseaux",
  "teddy bear": "ours-en-peluche",
  "hair drier": "s√®che-cheveux",
  "toothbrush": "brosse-√†-dents",
  "computer": "ordinateur",
  "tree": "arbre"
};

/**
 * Simule le chargement d'un mod√®le d'IA personnalis√© en compl√©ment de COCO-SSD
 * @returns {Promise<boolean>} √âtat du chargement du mod√®le
 */
const loadCustomModel = async () => {
  console.log("Chargement du mod√®le personnalis√©...");
  await new Promise(resolve => setTimeout(resolve, 1000)); // Simulation de chargement
  console.log("Mod√®le personnalis√© charg√©");
  return true;
};

/**
 * Traduit les noms des classes COCO-SSD en fran√ßais
 * @param {string} englishClass - Nom de classe en anglais
 * @returns {string} - Nom de classe en fran√ßais
 */
const translateClassName = (englishClass) => {
  return COCO_TO_FRENCH[englishClass] || englishClass;
};

/**
 * Enrichit les pr√©dictions de l'IA avec des informations suppl√©mentaires
 * @param {Array} predictions - Pr√©dictions brutes du mod√®le COCO-SSD
 * @returns {Array} - Pr√©dictions enrichies avec donn√©es additionnelles
 */
const enrichPredictions = (predictions) => {
  return predictions.map(prediction => {
    // D√©terminer si la pr√©diction vient du mod√®le YOLO ou COCO et traduire si n√©cessaire
    let className = prediction.class;
    let translatedClass = className;
    
    if (className.startsWith('custom-')) {
      // Utiliser notre liste personnalis√©e de noms de classes pour YOLO
      const classId = parseInt(className.replace('custom-', ''));
      if (classId >= 0 && classId < YOLO_CLASSES.length) {
        translatedClass = YOLO_CLASSES[classId];
      }
    } else {
      // Traduire les classes COCO-SSD
      translatedClass = translateClassName(className);
    }

    // On essaie d'abord de trouver la classe traduite dans objetInfos
    let baseInfo = objetInfos[translatedClass];
    
    // Si on ne trouve pas d'informations pour la classe traduite, on essaie avec la classe originale
    if (!baseInfo) {
      baseInfo = objetInfos[className];
    }
    
    // Si on ne trouve toujours pas, on utilise des valeurs par d√©faut
    if (!baseInfo) {
      baseInfo = {
        icon: className.includes('custom-') ? 'üîß' : '‚ùì',
        caracteristiques: "Informations non disponibles dans notre base de connaissances.",
        utilisation: "Utilisation non sp√©cifi√©e.",
        categories: ["non classifi√©"],
        materiaux: ["inconnu"],
        histoire: "Histoire non document√©e.",
        conseil: "Aucun conseil disponible.",
        dimensionsMoyennes: { notes: "Dimensions inconnues" },
        poidsEstime: "Inconnu",
        textePotentiel: "Texte inconnu"
      };
    }
    
    const [x, y, width, height] = prediction.bbox;
    const aspectRatio = width / height;
    
    // Estimation de la taille r√©elle (approximative)
    let tailleEstimee = null;
    if (baseInfo.dimensionsMoyennes) {
      tailleEstimee = {
        ...baseInfo.dimensionsMoyennes,
        ratioImage: aspectRatio.toFixed(2),
        surface: `${(width * height).toFixed(0)} pixels¬≤`,
        proportionImage: `${((width * height) / (640 * 480) * 100).toFixed(1)}% de l'image`
      };
    }
    
    return {
      ...prediction,
      class: translatedClass, // Utiliser le nom de classe traduit
      originalClass: className, // Conserver le nom de classe original
      ...baseInfo,
      detectedAt: new Date().toISOString(),
      certainty: prediction.score > 0.8 ? "√âlev√©e" : prediction.score > 0.6 ? "Moyenne" : "Faible",
      dimensions: {
        pixels: { width: width.toFixed(0), height: height.toFixed(0) },
        estimationTailleReelle: tailleEstimee
      },
      analyseTexte: {
        potentiel: baseInfo.textePotentiel,
        zoneTexte: width > 100 && height > 30 ? "Zone suffisante pour contenir du texte" : "Zone probablement trop petite pour du texte lisible"
      },
      source: prediction.class.includes('custom-') ? 'YOLO' : 'COCO-SSD'
    };
  });
};

/**
 * Analyse les dimensions et positions des objets d√©tect√©s dans l'image
 * @param {Object} prediction - Pr√©diction enrichie
 * @param {number} videoWidth - Largeur de la vid√©o
 * @param {number} videoHeight - Hauteur de la vid√©o
 * @returns {Object} - Analyse dimensionnelle compl√®te de l'objet
 */
const analyserDimensionsObjets = (prediction, videoWidth, videoHeight) => {
  const [x, y, width, height] = prediction.bbox;
  
  const proportionLargeur = width / videoWidth;
  const proportionHauteur = height / videoHeight;
  const proportionSurface = (width * height) / (videoWidth * videoHeight);
  
  // Estimation de distance bas√©e sur la taille relative
  let distanceEstimee = "ind√©termin√©e";
  if (proportionSurface > 0.5) distanceEstimee = "tr√®s proche";
  else if (proportionSurface > 0.25) distanceEstimee = "proche";
  else if (proportionSurface > 0.1) distanceEstimee = "distance moyenne";
  else if (proportionSurface > 0.05) distanceEstimee = "√©loign√©";
  else distanceEstimee = "tr√®s √©loign√©";
  
  return {
    proportionImage: {
      largeur: `${(proportionLargeur * 100).toFixed(1)}%`,
      hauteur: `${(proportionHauteur * 100).toFixed(1)}%`,
      surface: `${(proportionSurface * 100).toFixed(1)}%`
    },
    taillePixels: {
      largeur: Math.round(width),
      hauteur: Math.round(height),
      rapport: (width / height).toFixed(2)
    },
    distanceEstimee,
    dimensionsEstimees: prediction.dimensionsMoyennes || "Donn√©es non disponibles",
    position: {
      x: Math.round(x),
      y: Math.round(y),
      center: {
        x: Math.round(x + width/2),
        y: Math.round(y + height/2)
      },
      relativeCentre: {
        x: ((x + width/2) / videoWidth).toFixed(2),
        y: ((y + height/2) / videoHeight).toFixed(2)
      }
    }
  };
};

/**
 * Gestion intelligente des commentaires vocaux avec contr√¥le des d√©lais
 * et voix masculine
 */
class SpeechManager {
  constructor() {
    this.queue = [];
    this.speaking = false;
    this.lastMessages = {};
    this.cooldowns = {};
    this.enabled = true;
    this.audioContext = null;
    // Pour la synchronisation audio-vid√©o
    this.lastFrameTime = 0;
    this.frameDelayThreshold = 30; // ms
    // Voix pr√©f√©r√©e (masculine)
    this.preferredVoice = null;
  }

  // Initialisation du contexte audio pour une meilleure synchronisation
  initialize() {
    try {
      // Cr√©er un contexte audio seulement si le navigateur le supporte
      if (window.AudioContext || window.webkitAudioContext) {
        this.audioContext = new (window.AudioContext || window.webkitAudioContext)();
      }
      
      // Initialiser la voix masculine
      this.initializeVoice();
    } catch (error) {
      console.error("Impossible d'initialiser l'AudioContext:", error);
    }
  }
  
  // Recherche et configure une voix masculine en fran√ßais
  initializeVoice() {
    if (!window.speechSynthesis) return;
    
    // S'assurer que les voix sont charg√©es
    const checkVoices = () => {
      const voices = window.speechSynthesis.getVoices();
      if (voices.length > 0) {
        // Priorit√©: voix masculine fran√ßaise
        this.preferredVoice = voices.find(voice => 
          voice.lang.includes('fr') && voice.name.toLowerCase().includes('male'));
        
        // Si aucune voix masculine fran√ßaise n'est trouv√©e, essayer une voix fran√ßaise
        if (!this.preferredVoice) {
          this.preferredVoice = voices.find(voice => voice.lang.includes('fr'));
        }
        
        // Si toujours pas de voix, prendre une voix masculine quelconque
        if (!this.preferredVoice) {
          this.preferredVoice = voices.find(voice => voice.name.toLowerCase().includes('male'));
        }
        
        console.log("Voix s√©lectionn√©e:", this.preferredVoice ? this.preferredVoice.name : "Voix par d√©faut");
      } else {
        // R√©essayer si les voix ne sont pas encore charg√©es
        setTimeout(checkVoices, 100);
      }
    };
    
    // Chrome n√©cessite un √©v√©nement, Firefox non
    if (window.speechSynthesis.onvoiceschanged !== undefined) {
      window.speechSynthesis.onvoiceschanged = checkVoices;
    } else {
      checkVoices();
    }
  }

  setEnabled(enabled) {
    this.enabled = enabled;
    if (!enabled) {
      this.cancel();
    }
  }

  cancel() {
    window.speechSynthesis.cancel();
    this.queue = [];
    this.speaking = false;
  }

  // Utiliser cette m√©thode pour ajouter un message √† la file d'attente avec contr√¥le de synchronisation
  speak(text, priority = 1, objectId = null, frameTime = null) {
    if (!this.enabled) return;
    
    // Mise √† jour du timestamp de frame pour synchronisation
    if (frameTime) {
      const frameDelay = frameTime - this.lastFrameTime;
      this.lastFrameTime = frameTime;
      
      // Si le d√©lai entre frames est trop grand, on vide la file d'attente
      // pour √©viter des commentaires obsol√®tes
      if (frameDelay > this.frameDelayThreshold && this.queue.length > 0) {
        this.queue = this.queue.filter(item => item.priority >= 3); // Garder uniquement les messages prioritaires
      }
    }
    
    // √âviter les messages r√©p√©t√©s trop fr√©quemment pour le m√™me objet
    if (objectId) {
      const now = Date.now();
      const lastSpoken = this.lastMessages[objectId] || 0;
      const cooldown = this.cooldowns[objectId] || 2000; // 2 secondes par d√©faut
      
      if (now - lastSpoken < cooldown) {
        return; // Ignore ce message, trop r√©cent pour cet objet
      }
      
      this.lastMessages[objectId] = now;
      
      // Augmenter progressivement le cooldown pour √©viter trop de mises √† jour
      this.cooldowns[objectId] = Math.min(5000, cooldown + 500);
    }

    const message = { text, priority, timestamp: Date.now() };
    
    // Ajouter le message √† la file dans l'ordre de priorit√©
    const insertIndex = this.queue.findIndex(item => item.priority < priority);
    if (insertIndex === -1) {
      this.queue.push(message);
    } else {
      this.queue.splice(insertIndex, 0, message);
    }
    
    // Limiter la taille de la file pour √©viter accumulation
    if (this.queue.length > 5) {
      // Garder les messages prioritaires et les plus r√©cents
      this.queue = this.queue.sort((a, b) => {
        // D'abord par priorit√© (d√©croissante)
        if (b.priority !== a.priority) return b.priority - a.priority;
        // Puis par timestamp (croissant - plus r√©cent)
        return b.timestamp - a.timestamp;
      }).slice(0, 5);
    }
    
    this.processQueue();
  }

  // Traite la file de messages avec synchronisation am√©lior√©e
  processQueue() {
    if (this.speaking || this.queue.length === 0) return;
    
    const message = this.queue.shift();
    this.speaking = true;
    
    const utterance = new SpeechSynthesisUtterance(message.text);
    utterance.lang = 'fr-FR';
    utterance.rate = 1.1; // L√©g√®rement plus rapide pour une exp√©rience plus fluide
    
    // Appliquer la voix masculine si disponible
    if (this.preferredVoice) {
      utterance.voice = this.preferredVoice;
    }
    
    // Param√®tres pour voix masculine
    utterance.pitch = 0.9; // L√©g√®rement plus grave
    
    // Pr√©parer la synth√®se en avance pour r√©duire la latence
    if (this.audioContext) {
      // Cr√©er un petit son silencieux pour d√©bloquer l'audio si n√©cessaire
      const oscillator = this.audioContext.createOscillator();
      const gainNode = this.audioContext.createGain();
      gainNode.gain.value = 0; // Volume √† z√©ro (silencieux)
      oscillator.connect(gainNode);
      gainNode.connect(this.audioContext.destination);
      oscillator.start();
      oscillator.stop(this.audioContext.currentTime + 0.001);
    }
    
    utterance.onend = () => {
      this.speaking = false;
      // Attendre un court instant avant de traiter le message suivant
      setTimeout(() => this.processQueue(), 200);
    };
    
    // Gestion des erreurs de synth√®se vocale
    utterance.onerror = (event) => {
      console.error("Erreur de synth√®se vocale:", event);
      this.speaking = false;
      setTimeout(() => this.processQueue(), 200);
    };
    
    window.speechSynthesis.speak(utterance);
    
    // V√©rifier apr√®s un d√©lai si la synth√®se a commenc√©
    // Si non, on force la r√©initialisation pour √©viter le blocage
    setTimeout(() => {
      if (this.speaking && window.speechSynthesis.pending) {
        console.warn("Synth√®se vocale bloqu√©e, r√©initialisation...");
        window.speechSynthesis.cancel();
        this.speaking = false;
        setTimeout(() => this.processQueue(), 500);
      }
    }, 1000);
  }
}

/**
 * Composant principal d'analyse d'objets en temps r√©el
 */
const Analyse = () => {
  // R√©f√©rences
  const webcamRef = useRef(null);
  const canvasRef = useRef(null);
  const modelRef = useRef(null);
  const speechManagerRef = useRef(new SpeechManager());
  const animationFrameRef = useRef(null);

  // √âtats
  const [predictions, setPredictions] = useState([]);
  const [isDetecting, setIsDetecting] = useState(true);
  const [selectedObject, setSelectedObject] = useState(null);
  const [cameraFacingMode, setCameraFacingMode] = useState("environment");
  const [capturedImage, setCapturedImage] = useState(null);
  const [showSettings, setShowSettings] = useState(false);
  const [zoomLevel, setZoomLevel] = useState(1);
  const [brightness, setBrightness] = useState(100);
  const [audioEnabled, setAudioEnabled] = useState(true);
  const [detectionMode, setDetectionMode] = useState("normal");
  const [loadingModel, setLoadingModel] = useState(true);
  const [errorMessage, setErrorMessage] = useState(null);
  const [darkMode, setDarkMode] = useState(window.matchMedia('(prefers-color-scheme: dark)').matches);
  const [enableCloudAnalysis, setEnableCloudAnalysis] = useState(false);
  const [objectAnalyses, setObjectAnalyses] = useState({});
  const [lastSceneDescription, setLastSceneDescription] = useState("");
  const [aiLogs, setAiLogs] = useState([]); // √âtat pour les logs d'IA
  const [showAiLogs, setShowAiLogs] = useState(true); // Afficher/masquer les logs

  // R√©f√©rences pour optimisation
  const lastPredictionsRef = useRef([]);
  const objectHistoryRef = useRef({}); // Pour suivre l'historique de chaque objet d√©tect√©
  const lastDetectionTimeRef = useRef(0);
  const detectionInterval = detectionMode === "fast" ? 300 : detectionMode === "detail" ? 100 : 200;
  const lastDescriptionTimeRef = useRef(0);
  
  // Fonction pour ajouter des logs
  const addLog = useCallback((message, type = 'info') => {
    const timestamp = new Date().toLocaleTimeString();
    setAiLogs(prevLogs => {
      // Limiter √† 100 logs maximum
      const newLogs = [...prevLogs, { message, type, timestamp }];
      if (newLogs.length > 100) return newLogs.slice(-100);
      return newLogs;
    });
  }, []);
  
  // Appliquer le mode sombre
  useEffect(() => {
    document.body.classList.toggle('dark-mode', darkMode);
  }, [darkMode]);

  // Mettre √† jour l'√©tat d'activation audio du gestionnaire de parole
  useEffect(() => {
    speechManagerRef.current.setEnabled(audioEnabled);
  }, [audioEnabled]);

  // Initialiser le gestionnaire audio au d√©marrage
  useEffect(() => {
    speechManagerRef.current.initialize();
    addLog("Initialisation du gestionnaire audio", "system");
    
    // Nettoyer l'animation frame √† la fermeture pour √©viter les fuites m√©moire
    return () => {
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
      speechManagerRef.current.cancel();
    };
  }, [addLog]);

  // Optimisation du chargement des mod√®les et configuration TensorFlow
  useEffect(() => {
    const loadModelsAndData = async () => {
      try {
        setLoadingModel(true);
        addLog("D√©marrage du chargement des mod√®les", "system");
        
        // Configuration TensorFlow optimis√©e pour GPU
        await tf.ready();
        addLog("TensorFlow.js initialis√©", "system");
        
        // Optimisation de la m√©moire GPU/WebGL - valeurs √† ajuster selon les performances
        tf.env().set('WEBGL_FLUSH_THRESHOLD', 2); // R√©duire pour √©conomiser la m√©moire GPU
        tf.env().set('WEBGL_FORCE_F16_TEXTURES', true); // Utiliser des textures 16-bit pour √©conomiser la m√©moire
        tf.env().set('WEBGL_PACK', true); // Activer le packing pour optimiser les calculs
        tf.env().set('WEBGL_LAZILY_UNPACK', true); // D√©compresser paresseusement quand n√©cessaire
        
        // Activer WebGL explicitement
        await tf.setBackend('webgl');
        addLog(`Backend TensorFlow.js: ${tf.getBackend()}`, "success");
        
        // Chargement dynamique du fichier JSON avec les donn√©es des objets
        addLog("Chargement des donn√©es des objets...", "system");
        const module = await import('./objetInfos.json');
        objetInfos = module.default;
        addLog("Donn√©es des objets charg√©es avec succ√®s", "success");
        
        // Chargement des mod√®les en parall√®le
        addLog("Chargement des mod√®les d'IA...", "system");
        
        const modelPromises = [
          cocoSsd.load({
            base: detectionMode === "fast" ? "lite_mobilenet_v2" : "mobilenet_v2"
          })
        ];
        
        // Ajouter le chargement YOLO √† la liste des promesses
        try {
          addLog("Tentative de chargement du mod√®le YOLO personnalis√©...", "system");
          modelPromises.push(tf.loadGraphModel('/models/lifemodo_tfjs/model.json'));
        } catch (error) {
          addLog("Mod√®le YOLO non disponible, continuons sans lui", "warning");
        }
        
        // Attendre que tous les mod√®les soient charg√©s
        const loadedModels = await Promise.allSettled(modelPromises);
        
        // Stocker les mod√®les dans la r√©f√©rence
        modelRef.current = {
          cocoModel: loadedModels[0].status === 'fulfilled' ? loadedModels[0].value : null,
          yoloModel: loadedModels.length > 1 && loadedModels[1].status === 'fulfilled' ? loadedModels[1].value : null,
          customModelLoaded: loadedModels.length > 1 && loadedModels[1].status === 'fulfilled',
          createdAt: new Date()
        };
        
        if (!modelRef.current.cocoModel) {
          throw new Error("Impossible de charger le mod√®le COCO-SSD");
        }
        
        addLog("Mod√®le COCO-SSD charg√© avec succ√®s", "success");
        
        if (modelRef.current.yoloModel) {
          addLog("Mod√®le YOLO personnalis√© charg√© avec succ√®s", "success");
        }
        
        setLoadingModel(false);
        
        // Message de bienvenue avec d√©lai pour assurer la disponibilit√© audio
        setTimeout(() => {
          speechManagerRef.current.speak("Syst√®me d'analyse d'objets pr√™t √† l'emploi. Je suis votre assistant de d√©tection visuelle.", 3);
          addLog("Syst√®me d'IA pr√™t √† d√©tecter les objets", "success");
        }, 500);
      } catch (error) {
        console.error("Erreur lors du chargement:", error);
        addLog(`ERREUR: ${error.message}`, "error");
        setErrorMessage(`Impossible de charger les donn√©es ou les mod√®les d'IA. ${error.message || "Veuillez v√©rifier votre connexion internet et recharger l'application."}`);
        setLoadingModel(false);
      }
    };
    
    loadModelsAndData();
    
    return () => {
      // Nettoyage √† la fermeture
      speechManagerRef.current.cancel();
      addLog("Nettoyage des ressources...", "system");
      // Lib√©rer explicitement la m√©moire TensorFlow
      if (tf.getBackend() === 'webgl') {
        // @ts-ignore
        const gl = tf.backend().getGPGPUContext().gl;
        if (gl && typeof gl.getExtension === 'function') {
          // Forcer la lib√©ration des ressources WebGL
          const loseContextExt = gl.getExtension('WEBGL_lose_context');
          if (loseContextExt) loseContextExt.loseContext();
        }
      }
    };
  }, [detectionMode, addLog]);
  
    /**
   * G√©n√®re une description de la sc√®ne √† partir des objets d√©tect√©s
   * @param {Array} objets - Objets d√©tect√©s dans la sc√®ne
   * @returns {string} - Description textuelle de la sc√®ne
   */
  const genererDescriptionScene = useCallback((objets) => {
    if (!objets || objets.length === 0) return "Aucun objet d√©tect√© dans la sc√®ne.";

    // Grouper les objets par classe
    const groupes = {};
    objets.forEach(obj => {
      if (!groupes[obj.class]) groupes[obj.class] = [];
      groupes[obj.class].push(obj);
    });

    // Formater les groupes en texte
    const descriptions = Object.keys(groupes).map(classe => {
      const nombre = groupes[classe].length;
           const pluriel = nombre > 1 ? "s" : "";
      return `${nombre} ${classe}${pluriel}`;
    });

    // Construction de la phrase selon le nombre d'objets d√©tect√©s
    if (descriptions.length === 1) {
      return `D√©tect√©: ${descriptions[0]}.`;
    } else if (descriptions.length === 2) {
      return `D√©tect√©: ${descriptions[0]} et ${descriptions[1]}.`;
    } else {
      const dernier = descriptions.pop();
      return `D√©tect√©: ${descriptions.join(', ')} et ${dernier}.`;
    }
  }, []);

  /**
   * Pr√©pare et traite une image pour la d√©tection d'objets
   * @param {HTMLImageElement} image - √âl√©ment image √† analyser
   * @returns {tf.Tensor3D} - Tensor pr√™t pour l'inf√©rence
   */
  const preprocessImage = useCallback((image) => {
    // Conversion en tensor et normalisation
    const tensor = tf.browser.fromPixels(image);
    // Ajuster le zoom si n√©cessaire
    if (zoomLevel !== 1) {
      const [height, width] = tensor.shape;
      const centerHeight = height / 2;
      const centerWidth = width / 2;
      const newHeight = height / zoomLevel;
      const newWidth = width / zoomLevel;
      const startHeight = centerHeight - newHeight / 2;
      const startWidth = centerWidth - newWidth / 2;
      
      // Extraire la r√©gion zoom√©e
      const zoomedTensor = tf.image.cropAndResize(
        tf.expandDims(tensor),
        [[startHeight / height, startWidth / width, 
          (startHeight + newHeight) / height, (startWidth + newWidth) / width]],
        [0],
        [height, width]
      ).squeeze();
      
      tensor.dispose();
      return zoomedTensor;
    }
    return tensor;
  }, [zoomLevel]);

  /**
   * Pr√©traite et redimensionne l'image pour le mod√®le YOLO √† la taille attendue
   * @param {HTMLImageElement|HTMLVideoElement} image - √âl√©ment image ou vid√©o √† pr√©traiter
   * @param {Object} model - Mod√®le YOLO charg√©
   * @returns {tf.Tensor3D} - Tensor pr√™t pour l'inf√©rence
   */
  const preprocessImageForYOLO = useCallback((image, model) => {
    if (!model) return null;
    
    // R√©cup√©rer la taille d'entr√©e attendue par le mod√®le YOLO
    const inputShape = model.inputs[0].shape;
    // La forme est g√©n√©ralement [null, height, width, channels]
    const yoloWidth = inputShape[2];
    const yoloHeight = inputShape[1];
    
    // Conversion de l'image en tensor
    const tensor = tf.browser.fromPixels(image);
    
    // Redimensionner pour correspondre √† l'entr√©e du mod√®le YOLO
    const resized = tf.image.resizeBilinear(tensor, [yoloHeight, yoloWidth]);
    
    // Normaliser entre 0 et 1
    const normalized = resized.div(255.0);
    
    // Ajouter la dimension batch [1, height, width, 3]
    const batched = normalized.expandDims(0);
    
    // Nettoyer les tensors temporaires
    tensor.dispose();
    resized.dispose();
    normalized.dispose();
    
    return batched;
  }, []);

  /**
   * Traite les r√©sultats du mod√®le YOLO et les convertit au format compatible avec COCO-SSD
   * @param {tf.Tensor} output - Sortie du mod√®le YOLO
   * @param {number} imgWidth - Largeur originale de l'image
   * @param {number} imgHeight - Hauteur originale de l'image
   * @param {number} scoreThreshold - Seuil de confiance minimal pour les d√©tections
   * @returns {Array} - Pr√©dictions au format compatible avec COCO-SSD
   */
  const processYOLOOutput = useCallback((output, imgWidth, imgHeight, scoreThreshold = 0.25) => {
    // Le format de sortie de YOLO d√©pend de l'impl√©mentation exacte
    // Cet exemple suppose une sortie standard avec des bo√Ætes de d√©limitation, des scores de confiance et des classes
    
    // Convertir le tensor en tableau JavaScript
    const outputArray = Array.from(output.dataSync());
    const modelWidth = output.shape[2];
    const modelHeight = output.shape[1];
    const numClasses = (output.shape[3] - 5); // 5 = x, y, w, h, confidence
    
    const predictions = [];
    const numBoxes = outputArray.length / (numClasses + 5);
    
    for (let i = 0; i < numBoxes; i++) {
      const baseIndex = i * (numClasses + 5);
      const confidence = outputArray[baseIndex + 4];
      
      if (confidence > scoreThreshold) {
        // Trouver la classe avec le score le plus √©lev√©
        let maxClassScore = 0;
        let classIndex = 0;
        
        for (let c = 0; c < numClasses; c++) {
          const classScore = outputArray[baseIndex + 5 + c];
          if (classScore > maxClassScore) {
            maxClassScore = classScore;
            classIndex = c;
          }
        }
        
        const score = confidence * maxClassScore;
        
        if (score > scoreThreshold) {
          // R√©cup√©rer les coordonn√©es (format YOLO)
          let x = outputArray[baseIndex];
          let y = outputArray[baseIndex + 1];
          let w = outputArray[baseIndex + 2];
          let h = outputArray[baseIndex + 3];
          
          // Convertir de coordonn√©es normalis√©es √† pixels
          x = (x - w/2) * imgWidth;
          y = (y - h/2) * imgHeight;
          w = w * imgWidth;
          h = h * imgHeight;
          
          predictions.push({
            bbox: [x, y, w, h],
            class: `custom-${classIndex}`, // Pr√©fixe pour distinguer des classes COCO
            score: score,
            source: 'YOLO' // Marquer comme provenant de YOLO
          });
        }
      }
    }
    
    return predictions;
  }, []);

  /**
   * Fonction principale de d√©tection d'objets
   */
  const detectObjects = useCallback(async () => {
    // V√©rification des r√©f√©rences n√©cessaires
    if (!isDetecting || !webcamRef.current || !webcamRef.current.video || 
        !webcamRef.current.video.readyState === 4 || !modelRef.current || !modelRef.current.cocoModel) {
      return;
    }

    const video = webcamRef.current.video;
    const videoWidth = video.videoWidth;
    const videoHeight = video.videoHeight;
    
    // Ne pas ex√©cuter la d√©tection si l'intervalle depuis la derni√®re d√©tection est trop court
    const now = Date.now();
    if (now - lastDetectionTimeRef.current < detectionInterval) {
      // Planifier la prochaine frame d'animation
      animationFrameRef.current = requestAnimationFrame(detectObjects);
      return;
    }
    lastDetectionTimeRef.current = now;
    
    try {
      // Pr√©traitement d'image avec TensorFlow.js
      const tensor = preprocessImage(video);
      
      // D√©tection avec le mod√®le COCO-SSD
      const cocoResults = await modelRef.current.cocoModel.detect(tensor);

      // Lib√©rer la m√©moire du tensor apr√®s utilisation
      tensor.dispose();
      
      // Combiner les r√©sultats des diff√©rents mod√®les
      let allResults = [...cocoResults];
      
      // Ajouter les pr√©dictions du mod√®le YOLO si disponible
      if (modelRef.current.yoloModel) {
        try {
          // Pr√©traiter l'image sp√©cifiquement pour YOLO
          const yoloTensor = preprocessImageForYOLO(video, modelRef.current.yoloModel);
          
          if (yoloTensor) {
            // Ex√©cuter l'inf√©rence avec le mod√®le YOLO
            const yoloOutput = modelRef.current.yoloModel.predict(yoloTensor);
            
            // Traiter les r√©sultats de YOLO et les convertir en format compatible
            const yoloResults = processYOLOOutput(yoloOutput, videoWidth, videoHeight);
            
            // Lib√©rer la m√©moire
            yoloTensor.dispose();
            yoloOutput.dispose();
            
            // Ajouter les r√©sultats de YOLO √† nos pr√©dictions
            allResults = [...allResults, ...yoloResults];
            
            addLog(`YOLO a d√©tect√© ${yoloResults.length} objets`, "info");
          }
        } catch (yoloError) {
          console.error("Erreur lors de la d√©tection YOLO:", yoloError);
          addLog(`Erreur YOLO: ${yoloError.message}`, "error");
        }
      }
      
      // Enrichir les pr√©dictions avec des informations suppl√©mentaires
      const enrichedPredictions = enrichPredictions(allResults);
      
      // Mettre √† jour l'√©tat avec les nouvelles pr√©dictions si elles sont diff√©rentes
      const predictionsChanged = JSON.stringify(enrichedPredictions) !== JSON.stringify(lastPredictionsRef.current);
      
      if (predictionsChanged) {
        // Mise √† jour de la r√©f√©rence et de l'√©tat
        lastPredictionsRef.current = enrichedPredictions;
        setPredictions(enrichedPredictions);
        
        // Mettre √† jour les analyses dimensionnelles pour chaque objet
        const analysesObj = {};
        enrichedPredictions.forEach(pred => {
          analysesObj[pred.class + '-' + pred.bbox.join(',')] = analyserDimensionsObjets(pred, videoWidth, videoHeight);
        });
        setObjectAnalyses(analysesObj);
        
        // Mettre √† jour la description de la sc√®ne toutes les 3 secondes
        if (now - lastDescriptionTimeRef.current > 3000) {
          const description = genererDescriptionScene(enrichedPredictions);
          setLastSceneDescription(description);
          lastDescriptionTimeRef.current = now;
          
          // Annoncer vocalement si audio activ√©
          if (audioEnabled && enrichedPredictions.length > 0) {
            speechManagerRef.current.speak(description, 2, 'scene-description', now);
          }
          
          addLog(`Analyse compl√®te: ${description}`, "info");
        }
        
        // Annoncer vocalement les nouveaux objets avec forte confiance
        if (audioEnabled) {
          enrichedPredictions
            .filter(pred => pred.score > 0.7)
            .forEach(pred => {
              const objectId = pred.class;
              // V√©rifier si c'est un nouvel objet ou s'il n'a pas √©t√© annonc√© r√©cemment
              if (!objectHistoryRef.current[objectId] || now - objectHistoryRef.current[objectId] > 5000) {
                const message = `${pred.icon || ''} ${pred.class} d√©tect√©${pred.class.endsWith('e') ? 'e' : ''}.`;
                speechManagerRef.current.speak(message, 1, objectId, now);
                objectHistoryRef.current[objectId] = now;
              }
            });
        }
      }
      
      // Dessiner les rectangles de d√©tection sur le canvas
      if (canvasRef.current) {
        const canvas = canvasRef.current;
        const ctx = canvas.getContext("2d");
        // Ajuster les dimensions du canvas √† celles de la vid√©o
        canvas.width = videoWidth;
        canvas.height = videoHeight;
        
        // Effacer le canvas pr√©c√©dent
        ctx.clearRect(0, 0, ctx.canvas.width, ctx.canvas.height);
        
        // Appliquer les ajustements de luminosit√© au canvas
        if (brightness !== 100) {
          ctx.filter = `brightness(${brightness}%)`;
        }
        
        // Dessiner la vid√©o d'abord
        ctx.drawImage(video, 0, 0, videoWidth, videoHeight);
        
        // R√©initialiser les filtres pour les d√©tections
        ctx.filter = 'none';
        
        // Dessiner chaque d√©tection
        enrichedPredictions.forEach((prediction) => {
          // Extraire les coordonn√©es et dimensions
          const [x, y, width, height] = prediction.bbox;
          
          // D√©finir le style du rectangle
          const isSelected = selectedObject && 
            selectedObject.class === prediction.class && 
            JSON.stringify(selectedObject.bbox) === JSON.stringify(prediction.bbox);
          
          // Couleur bas√©e sur la source et la confiance
          let strokeColor = "rgba(0, 255, 0, 0.8)"; // Vert pour COCO-SSD par d√©faut

          if (isSelected) {
            strokeColor = "rgba(255, 215, 0, 0.9)"; // Or pour la s√©lection
          } else if (prediction.source === 'YOLO') {
            strokeColor = "rgba(255, 165, 0, 0.8)"; // Orange pour YOLO
          } else if (prediction.score < 0.6) {
            strokeColor = "rgba(255, 69, 0, 0.8)"; // Rouge orang√© pour faible confiance
          }
          
          // Dessiner le rectangle
          ctx.strokeStyle = strokeColor;
          ctx.lineWidth = isSelected ? 4 : 2;
          ctx.strokeRect(x, y, width, height);
          
          // Fond semi-transparent pour le texte
          const textWidth = ctx.measureText(`${prediction.class} (${Math.round(prediction.score * 100)}%)`).width + 10;
          ctx.fillStyle = "rgba(0, 0, 0, 0.6)";
          ctx.fillRect(x, y - 22, textWidth, 22);
          
          // Texte d'identification
          ctx.fillStyle = "white";
          ctx.font = "16px Arial";
          ctx.fillText(
            `${prediction.class} (${Math.round(prediction.score * 100)}%)`, 
            x + 5, 
            y - 5
          );
          
          // Ajouter un indicateur d'ic√¥ne si disponible
          if (prediction.icon) {
            ctx.font = "18px Arial";
            ctx.fillText(prediction.icon, x + width - 25, y - 5);
          }
          
          // Si l'objet est s√©lectionn√©, ajouter des informations suppl√©mentaires
          if (isSelected) {
            const infoY = y + height + 20;
            ctx.fillStyle = "rgba(0, 0, 0, 0.7)";
            ctx.fillRect(x, infoY, width, 70);
            ctx.fillStyle = "white";
            ctx.font = "14px Arial";
            
            // Afficher les dimensions
            ctx.fillText(`Dim: ${Math.round(width)}√ó${Math.round(height)} px`, x + 5, infoY + 18);
            
            // Afficher les cat√©gories
            if (prediction.categories && prediction.categories.length > 0) {
              ctx.fillText(`Type: ${prediction.categories.join(', ')}`, x + 5, infoY + 38);
            }
            
            // Afficher une info d'utilisation courte
            if (prediction.utilisation) {
              const shortUsage = prediction.utilisation.length > 40 
                ? prediction.utilisation.substring(0, 40) + '...' 
                : prediction.utilisation;
              ctx.fillText(`Usage: ${shortUsage}`, x + 5, infoY + 58);
            }
          }
        });
      }
    } catch (error) {
      console.error("Erreur lors de la d√©tection:", error);
      addLog(`Erreur de d√©tection: ${error.message}`, "error");
    }
    
    // Planifier la prochaine frame d'animation
    animationFrameRef.current = requestAnimationFrame(detectObjects);
  }, [isDetecting, detectionInterval, brightness, zoomLevel, selectedObject, audioEnabled, detectionMode, 
      genererDescriptionScene, preprocessImage, preprocessImageForYOLO, processYOLOOutput, addLog]);

  // D√©marrer la d√©tection lorsque le composant est mont√©
  useEffect(() => {
    if (!loadingModel && !errorMessage) {
      detectObjects();
    }
    return () => {
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
    };
  }, [loadingModel, errorMessage, detectObjects]);

  // Capturer une image de la webcam
  const captureImage = useCallback(() => {
    if (webcamRef.current) {
      const imageSrc = webcamRef.current.getScreenshot();
      setCapturedImage(imageSrc);
      setIsDetecting(false); // Arr√™ter la d√©tection pendant la visualisation de l'image
      
      // Annoncer la capture vocalement
      if (audioEnabled) {
        speechManagerRef.current.speak("Image captur√©e. Analyse en cours.", 3);
      }
      
      // Simuler une analyse approfondie sur l'image captur√©e
      addLog("Analyse d√©taill√©e de l'image captur√©e...", "processing");
      
      // Cr√©er une image pour l'analyse
      const img = new Image();
      img.src = imageSrc;
      img.onload = async () => {
        try {
          // Analyse avec le mod√®le COCO-SSD
          const tensor = preprocessImage(img);
          const cocoResults = await modelRef.current.cocoModel.detect(tensor);
          tensor.dispose();
          
          // Combiner les r√©sultats des diff√©rents mod√®les
          let allResults = [...cocoResults];
          
          // Ajouter les pr√©dictions du mod√®le YOLO si disponible
          if (modelRef.current.yoloModel) {
            try {
              // Pr√©traiter l'image sp√©cifiquement pour YOLO
              const yoloTensor = preprocessImageForYOLO(img, modelRef.current.yoloModel);
              
              if (yoloTensor) {
                // Ex√©cuter l'inf√©rence avec le mod√®le YOLO
                const yoloOutput = modelRef.current.yoloModel.predict(yoloTensor);
                
                // Traiter les r√©sultats de YOLO et les convertir en format compatible
                const yoloResults = processYOLOOutput(yoloOutput, img.width, img.height);
                
                // Lib√©rer la m√©moire
                yoloTensor.dispose();
                yoloOutput.dispose();
                
                // Ajouter les r√©sultats de YOLO √† nos pr√©dictions
                allResults = [...allResults, ...yoloResults];
                
                addLog(`YOLO a d√©tect√© ${yoloResults.length} objets dans l'image captur√©e`, "info");
              }
            } catch (yoloError) {
              console.error("Erreur lors de la d√©tection YOLO:", yoloError);
              addLog(`Erreur YOLO: ${yoloError.message}`, "error");
            }
          }
          
          // Enrichir les r√©sultats
          const enrichedResults = enrichPredictions(allResults);
          setPredictions(enrichedResults);
          
          // Mettre √† jour les analyses dimensionnelles
          const analysesObj = {};
          enrichedResults.forEach(pred => {
            analysesObj[pred.class + '-' + pred.bbox.join(',')] = analyserDimensionsObjets(
              pred, img.width, img.height
            );
          });
          setObjectAnalyses(analysesObj);
          
          // G√©n√©rer une description
          const description = genererDescriptionScene(enrichedResults);
          setLastSceneDescription(description);
          
          // Annoncer les r√©sultats vocalement
          if (audioEnabled) {
            speechManagerRef.current.speak(
              `Analyse termin√©e. ${description}`,
              3, 'image-analysis'
            );
          }
          
          addLog(`Analyse de l'image termin√©e: ${description}`, "success");
        } catch (error) {
          console.error("Erreur lors de l'analyse de l'image captur√©e:", error);
          addLog(`Erreur d'analyse: ${error.message}`, "error");
        }
      };
    }
  }, [webcamRef, audioEnabled, setIsDetecting, preprocessImage, preprocessImageForYOLO, processYOLOOutput, analyserDimensionsObjets, genererDescriptionScene, addLog]);

  // Retourner √† la vue webcam depuis l'image captur√©e
  const retourWebcam = useCallback(() => {
    setCapturedImage(null);
    setIsDetecting(true);
    addLog("Retour au mode d√©tection en temps r√©el", "info");
  }, [addLog]);

  // Fonction pour changer la cam√©ra (avant/arri√®re)
  const toggleCamera = useCallback(() => {
    setCameraFacingMode(prevMode => 
      prevMode === "user" ? "environment" : "user"
    );
    addLog(`Basculement vers la cam√©ra ${cameraFacingMode === "user" ? "arri√®re" : "avant"}`, "info");
  }, [cameraFacingMode, addLog]);

  // G√©rer la s√©lection d'un objet d√©tect√©
  const handleObjectClick = useCallback((event) => {
    // Ne rien faire si on est en mode image captur√©e sans d√©tection
    if (!isDetecting && !capturedImage) return;
    
    const canvas = canvasRef.current;
    if (!canvas) return;
    
    // Obtenir les coordonn√©es du clic relatif au canvas
    const rect = canvas.getBoundingClientRect();
    const x = (event.clientX - rect.left) * (canvas.width / rect.width);
    const y = (event.clientY - rect.top) * (canvas.height / rect.height);
    
    // V√©rifier si le clic est sur un objet d√©tect√©
    let clickedObject = null;
    
    // Parcourir les pr√©dictions de la plus petite √† la plus grande (pour g√©rer le cas o√π un petit objet est √† l'int√©rieur d'un grand)
    [...predictions]
      .sort((a, b) => {
        const areaA = a.bbox[2] * a.bbox[3];
        const areaB = b.bbox[2] * b.bbox[3];
        return areaA - areaB; // Trier du plus petit au plus grand
      })
      .forEach(prediction => {
        const [objX, objY, objWidth, objHeight] = prediction.bbox;
        if (x >= objX && x <= objX + objWidth && y >= objY && y <= objY + objHeight) {
          clickedObject = prediction;
        }
      });
    
    // Mettre √† jour l'objet s√©lectionn√©
    setSelectedObject(clickedObject);
    
    // Annoncer vocalement l'objet s√©lectionn√©
    if (clickedObject && audioEnabled) {
      const objectName = clickedObject.class;
      const message = `${objectName} s√©lectionn√©${objectName.endsWith('e') ? 'e' : ''}.`;
      speechManagerRef.current.speak(message, 2, 'selection');
      
      // Ajouter un message informatif suppl√©mentaire bas√© sur les caract√©ristiques
      if (clickedObject.caracteristiques) {
        const infoShort = clickedObject.caracteristiques.split('.')[0]; // Premi√®re phrase seulement
        setTimeout(() => {
          speechManagerRef.current.speak(infoShort, 1, 'info-object');
        }, 1000);
      }
      
      addLog(`Objet s√©lectionn√©: ${objectName}`, "info");
    } else if (!clickedObject) {
      addLog("Aucun objet s√©lectionn√© √† cet emplacement", "info");
    }
  }, [predictions, isDetecting, capturedImage, audioEnabled, addLog]);

  // G√©rer le t√©l√©chargement de l'image analys√©e
  const handleSaveImage = useCallback(() => {
    const canvas = canvasRef.current;
    if (!canvas) return;
    
    // Cr√©er un lien de t√©l√©chargement
    const link = document.createElement('a');
    link.download = `analyse-objets-${new Date().toISOString().slice(0,19).replace(/:/g, '-')}.png`;
    
    // Convertir le canvas en URL de donn√©es
    link.href = canvas.toDataURL('image/png');
    document.body.appendChild(link);
    link.click();
    document.body.removeChild(link);
    
    addLog("Image sauvegard√©e", "success");
    
    // Confirmation vocale
    if (audioEnabled) {
      speechManagerRef.current.speak("Image enregistr√©e avec les annotations d'analyse.", 2);
    }
  }, [audioEnabled, addLog]);

  // Conteneur pour les contraintes vid√©o
  const videoConstraints = {
    width: 640,
    height: 480,
    facingMode: cameraFacingMode,
    aspectRatio: 1.333,
  };

  // Param√®tres de style CSS pour le zoom et la luminosit√©
  const webcamStyle = {
    transform: `scale(${zoomLevel})`,
    filter: `brightness(${brightness}%)`,
    transformOrigin: 'center',
  };

  // Rendu du composant
  return (
    <div className="analyse-container">
      {loadingModel ? (
        <div className="loading-container">
          <div className="loading-spinner"></div>
          <h2>Chargement des mod√®les d'IA...</h2>
          <p>Pr√©paration des capacit√©s de d√©tection d'objets et d'analyse.</p>
          <div className="progress-bar">
            <div className="progress-fill"></div>
          </div>
        </div>
      ) : errorMessage ? (
        <div className="error-container">
          <h2>Erreur</h2>
          <p>{errorMessage}</p>
          <button onClick={() => window.location.reload()}>
            Recharger l'application
          </button>
        </div>
      ) : (
        <>
          <div className="camera-container">
            {capturedImage ? (
              <img
                src={capturedImage}
                alt="Captured"
                style={{ width: '100%', height: 'auto' }}
              />
            ) : (
              <Webcam
                audio={false}
                ref={webcamRef}
                screenshotFormat="image/jpeg"
                videoConstraints={videoConstraints}
                style={webcamStyle}
                className="webcam"
                mirrored={cameraFacingMode === "user"}
              />
            )}
            
            <canvas
              ref={canvasRef}
              className="detection-canvas"
              onClick={handleObjectClick}
            />
            
            <div className="scene-description">
              <FaRobot className="icon" />
              <span>{lastSceneDescription || "Aucun objet d√©tect√©."}</span>
            </div>
          </div>
          
          <div className="controls">
            <button 
              className="control-button"
              onClick={capturedImage ? retourWebcam : captureImage}
              title={capturedImage ? "Retour √† la cam√©ra" : "Capturer l'image"}
            >
              {capturedImage ? <MdPhotoLibrary /> : <FaCamera />}
            </button>
            
            <button 
              className="control-button"
              onClick={handleSaveImage}
              title="Enregistrer l'image avec annotations"
            >
              <FaSave />
            </button>
            
            <button 
              className="control-button"
              onClick={toggleCamera}
              title="Changer de cam√©ra"
            >
              <FaExchangeAlt />
            </button>
            
            <button 
              className="control-button"
              onClick={() => setAudioEnabled(!audioEnabled)}
              title={audioEnabled ? "D√©sactiver l'audio" : "Activer l'audio"}
            >
              {audioEnabled ? <FaVolumeUp /> : <FaVolumeMute />}
            </button>
            
            <button 
              className="control-button"
              onClick={() => setShowSettings(!showSettings)}
              title="Param√®tres"
            >
              <FaCog />
            </button>
            
            <button 
              className="control-button"
              onClick={() => setShowAiLogs(!showAiLogs)}
              title={showAiLogs ? "Masquer logs IA" : "Afficher logs IA"}
            >
              <FaBrain />
            </button>
          </div>
          
          <AnimatePresence>
            {showSettings && (
              <motion.div 
                className="settings-panel"
                initial={{ opacity: 0, y: 20 }}
                animate={{ opacity: 1, y: 0 }}
                exit={{ opacity: 0, y: 20 }}
              >
                <h3>Param√®tres</h3>
                
                <div className="setting-group">
                  <label>Zoom: {zoomLevel.toFixed(1)}x</label>
                  <div className="setting-controls">
                    <button onClick={() => setZoomLevel(Math.max(1, zoomLevel - 0.1))}>
                      <BsZoomOut />
                    </button>
                    <input
                      type="range"
                      min="1"
                      max="3"
                      step="0.1"
                      value={zoomLevel}
                      onChange={(e) => setZoomLevel(parseFloat(e.target.value))}
                    />
                    <button onClick={() => setZoomLevel(Math.min(3, zoomLevel + 0.1))}>
                      <BsZoomIn />
                    </button>
                  </div>
                </div>
                
                <div className="setting-group">
                  <label>Luminosit√©: {brightness}%</label>
                  <input
                    type="range"
                    min="50"
                    max="150"
                    value={brightness}
                    onChange={(e) => setBrightness(parseInt(e.target.value))}
                  />
                </div>
                
                <div className="setting-group">
                  <label>Mode de d√©tection:</label>
                  <select 
                    value={detectionMode}
                    onChange={(e) => setDetectionMode(e.target.value)}
                  >
                    <option value="fast">Rapide (moins pr√©cis)</option>
                    <option value="normal">Normal (√©quilibr√©)</option>
                                       <option value="detail">D√©taill√© (plus lent)</option>
                  </select>
                </div>
                
                <div className="setting-group">
                  <label>
                    <input
                      type="checkbox"
                      checked={darkMode}
                      onChange={() => setDarkMode(!darkMode)}
                    />
                    Mode sombre
                  </label>
                </div>
                
                <div className="setting-group">
                  <label>
                    <input
                      type="checkbox"
                      checked={enableCloudAnalysis}
                      onChange={() => setEnableCloudAnalysis(!enableCloudAnalysis)}
                    />
                    Analyse cloud (plus pr√©cise)
                  </label>
                </div>
                
                <button onClick={() => setShowSettings(false)}>Fermer</button>
              </motion.div>
            )}
          </AnimatePresence>
          
          {/* Panel d'informations d√©taill√©es sur l'objet s√©lectionn√© */}
          <AnimatePresence>
            {selectedObject && (
              <motion.div 
                className="object-info-panel"
                initial={{ opacity: 0, x: 20 }}
                animate={{ opacity: 1, x: 0 }}
                exit={{ opacity: 0, x: 20 }}
              >
                <h3>
                  {selectedObject.icon && <span className="object-icon">{selectedObject.icon} </span>}
                  {selectedObject.class}
                  <span className="confidence-badge" style={{ 
                    backgroundColor: selectedObject.score > 0.8 ? '#4caf50' : selectedObject.score > 0.6 ? '#ff9800' : '#f44336' 
                  }}>
                    {Math.round(selectedObject.score * 100)}%
                  </span>
                  <button className="close-button" onClick={() => setSelectedObject(null)}>√ó</button>
                </h3>
                
                <div className="info-source">
                  <small>D√©tect√© par: {selectedObject.source || 'COCO-SSD'}</small>
                </div>
                
                <div className="object-info-content">
                  {selectedObject.caracteristiques && (
                    <div className="info-section">
                      <h4>Caract√©ristiques</h4>
                      <p>{selectedObject.caracteristiques}</p>
                    </div>
                  )}
                  
                  {selectedObject.utilisation && (
                    <div className="info-section">
                      <h4>Utilisation</h4>
                      <p>{selectedObject.utilisation}</p>
                    </div>
                  )}
                  
                  {selectedObject.categories && selectedObject.categories.length > 0 && (
                    <div className="info-section">
                      <h4>Cat√©gories</h4>
                      <div className="tags">
                        {selectedObject.categories.map((cat, index) => (
                          <span key={index} className="tag">{cat}</span>
                        ))}
                      </div>
                    </div>
                  )}
                  
                  {selectedObject.materiaux && selectedObject.materiaux.length > 0 && (
                    <div className="info-section">
                      <h4>Mat√©riaux courants</h4>
                      <div className="tags">
                        {selectedObject.materiaux.map((mat, index) => (
                          <span key={index} className="tag">{mat}</span>
                        ))}
                      </div>
                    </div>
                  )}
                  
                  {objectAnalyses[selectedObject.class + '-' + selectedObject.bbox.join(',')] && (
                    <div className="info-section">
                      <h4>Analyse dimensionnelle</h4>
                      <div className="dimension-info">
                        <p>
                          <strong>Dimensions (px):</strong> {objectAnalyses[selectedObject.class + '-' + selectedObject.bbox.join(',')].taillePixels.largeur} √ó {objectAnalyses[selectedObject.class + '-' + selectedObject.bbox.join(',')].taillePixels.hauteur}
                        </p>
                        <p>
                          <strong>Proportion:</strong> {objectAnalyses[selectedObject.class + '-' + selectedObject.bbox.join(',')].proportionImage.surface}
                        </p>
                        <p>
                          <strong>Distance estim√©e:</strong> {objectAnalyses[selectedObject.class + '-' + selectedObject.bbox.join(',')].distanceEstimee}
                        </p>
                      </div>
                    </div>
                  )}
                  
                  {selectedObject.conseil && (
                    <div className="info-section">
                      <h4>Conseil</h4>
                      <p className="tip">{selectedObject.conseil}</p>
                    </div>
                  )}
                </div>
              </motion.div>
            )}
          </AnimatePresence>
          
          {/* Panneau des logs d'IA */}
          <AnimatePresence>
            {showAiLogs && (
              <motion.div 
                className="ai-logs-panel"
                initial={{ opacity: 0, y: 20 }}
                animate={{ opacity: 1, y: 0 }}
                exit={{ opacity: 0, y: 20 }}
              >
                <div className="logs-header">
                  <h3><FaBrain /> Logs d'IA</h3>
                  <button onClick={() => setShowAiLogs(false)}>√ó</button>
                </div>
                <div className="logs-content">
                  {aiLogs.map((log, index) => (
                    <div key={index} className={`log-entry log-${log.type}`}>
                      <span className="log-time">{log.timestamp}</span>
                      <span className="log-message">{log.message}</span>
                    </div>
                  ))}
                </div>
              </motion.div>
            )}
          </AnimatePresence>
        </>
      )}
    </div>
  );
};

export default Analyse;

